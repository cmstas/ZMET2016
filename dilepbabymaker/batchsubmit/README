--------------------------------------------------
--- RUNNING ALL HADRONIC MT2 LOOPER IN BATCH MODE ---
--------------------------------------------------

0) environment setup: see MT2Analysis/README.txt.  This setup sends the compiled libraries with the job so assumes that you're using a particular CMSSW/root version.

1) copy files to run looper into job_input directory
for example, from this directory
> cp -r ../*.so ../jetCorrections ../processBaby job_input/

*** note that after copying the latest source files into the job_input subdir, you must run one of the writeConfig scripts below to recreate the input tarball which is used for the batch jobs!

2) modify writeConfig.sh script for personal setup. Variables that should be modified by the user are
PROXY (not nedded anymore unless you have a special location for your proxy file)
COPYDIR to point to the desired output directory in hadoop ( default to /hadoop/cms/store/user/${USERNAME}/mt2babies/) where $USERNAME is your username

3) now execute scripts!
to run on only one dataset for example run the writeConfig.sh script 
this takes two arguments, the dataset directory on hadoop and the name
you want to give the output babies and the output directory. 
for example
> ./writeConfig.sh /hadoop/cms/store/group/snt/csa14/MC_CMS3_V07-00-03/TTJets_MSDecaysCKM_central_Tune4C_13TeV-madgraph-tauola/merged/ V00-00-01_TTJets
creates a condor config file
condor_V00-00-01_TTJets.cmd
to submit one job per file in the dataset directory 

Note, need to setup voms proxy since apparently this is needed to copy output files at the end of the job.
something like
> voms-proxy-init -voms cms -valid 240:00

now submit to condor
> condor_submit <CMDFILE>

check status 
> condor_q <USERNAME>

if jobs are listed as held ("H"):
> condor_release <USERNAME>

select datasets to run on in writeAllConfig.sh and execute
> ./writeAllConfig.sh
This should create a set of .cmd files and a submit script submitAll.sh
> ./submitAll.sh

the job .out and .err files should be located in the job_logs directory 
the submission log should be in submit_logs

4) To verify that jobs ran and produced all the output files, first run sweepRoot to check all output root files.

> git clone git@github.com:cmstas/NtupleTools.git
> cd NtupleTools/condorMergingTools
> make
> ./sweepRoot -b -o "t" /hadoop/cms/store/user/${USER}/dilepbabies/<BABYDIRS>/*.root

where <BABYDIRS> is something like V00-00-01_*

Delete the files reported as bad, then run checkAllConfig to see which output files are missing:

> ./checkAllConfig.sh <CONFIGDIR>

where <CONFIGDIR> is something like configs_V00-00-01/

This creates resubmit configs for just the missing jobs, with names *_resubmit.cmd, in <CONFIGDIR>.  

To create the resubmit file for a single config:
> ./resubmitConfig.sh <CMDFILE>

Then submit the jobs again as before:
> condor_submit <NEWCMDFILE>

You may have to iterate this process to get all jobs to converge.

5) Once the jobs are done, merge the output with the mergeHadoopOutput script
You can select which datasets to merge by editing the script below, then run it.
Note that all the merge jobs run locally in parallel, so you may not want to merge
everything at once
> ./mergeHadoopFiles.sh
You can also specify the output directory in the script. Typically the babies are
stored under:
/nfs-6/userdata/ZMETbabies/<VERSION>/
